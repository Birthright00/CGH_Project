import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr

# Load tokenizer and model
model_id = "RedHatAI/DeepSeek-R1-Distill-Llama-8B-quantized.w8a8"

print("ðŸ”„ Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",  # Use GPU if available
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    trust_remote_code=True
)
print("âœ… Model loaded successfully.")

# Response generation function
def generate_response(prompt):
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=300,
            do_sample=False,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            eos_token_id=tokenizer.eos_token_id,
        )

    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return response[len(prompt):].strip()

# Optional default email example
default_prompt = """Extract and label the following email into this format:
Original Session:
Change Request:
Reason:
Students:

Email:
Dear Jeffrey,
I am unable to make it for May 15 class as I have a sudden medical appointment with a patient.
Please change the class to May 25 4-6pm instead.
Thank you.

From,
Dr Who
"""

# Launch Gradio UI
interface = gr.Interface(
    fn=generate_response,
    inputs=gr.Textbox(lines=15, value=default_prompt, label="Prompt / Email"),
    outputs=gr.Textbox(label="Model Response"),
    title="ðŸ§  DeepSeek-R1-Distill-Qwen-1.5B Email Assistant",
    description="Paste your email prompt and get structured output using DeepSeek LLM.",
)

interface.launch()
